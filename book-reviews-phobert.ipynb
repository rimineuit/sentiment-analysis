{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10219681,"sourceType":"datasetVersion","datasetId":6317549}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, classification_report\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport numpy as np\nfrom typing import *\nimport json\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pad_sequence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:41:57.116452Z","iopub.execute_input":"2024-12-16T19:41:57.116898Z","iopub.status.idle":"2024-12-16T19:42:01.509870Z","shell.execute_reply.started":"2024-12-16T19:41:57.116844Z","shell.execute_reply":"2024-12-16T19:42:01.508993Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\n\nclass BookReviews(Dataset):\n    def __init__(self, path: str, tokenizer, max_length=256):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n        # Load data\n        data = pd.read_json(path, orient=\"records\", lines=True)\n        self.sentences = data[\"review\"].tolist()\n        self.labels = data[\"sentiment\"].tolist()\n\n        # Create label-to-index mapping\n        labels_unique = set(self.labels)\n        self.labels_to_idx = {label: i for i, label in enumerate(labels_unique)}\n\n    def __getitem__(self, idx):\n        # Tokenize sentence\n        encoded = self.tokenizer(\n            self.sentences[idx],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        \n        # Get label\n        label = self.labels_to_idx[self.labels[idx]]\n        \n        # Return dictionary\n        return {\n            \"input_ids\": encoded[\"input_ids\"].squeeze(0),  # Remove batch dimension\n            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(label, dtype=torch.long)  # Change 'label' to 'labels'\n        }\n\n    \n    def __len__(self):\n        return len(self.sentences)\n    \n    def num_labels(self):\n        return len(self.labels_to_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:06.271071Z","iopub.execute_input":"2024-12-16T19:46:06.271856Z","iopub.status.idle":"2024-12-16T19:46:06.279669Z","shell.execute_reply.started":"2024-12-16T19:46:06.271819Z","shell.execute_reply":"2024-12-16T19:46:06.278578Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from transformers import RobertaForSequenceClassification, AutoTokenizer\n\nmodel = RobertaForSequenceClassification.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"wonrax/phobert-base-vietnamese-sentiment\", use_fast=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:14.535192Z","iopub.execute_input":"2024-12-16T19:46:14.535522Z","iopub.status.idle":"2024-12-16T19:46:15.103348Z","shell.execute_reply.started":"2024-12-16T19:46:14.535494Z","shell.execute_reply":"2024-12-16T19:46:15.102594Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"train_dataset = BookReviews(\n    path='/kaggle/input/new-data/new_data/train.json',\n    tokenizer=tokenizer\n)\ntest_dataset = BookReviews(\n    path='/kaggle/input/new-data/new_data/test.json',\n    tokenizer=tokenizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:15.980492Z","iopub.execute_input":"2024-12-16T19:46:15.981303Z","iopub.status.idle":"2024-12-16T19:46:16.261712Z","shell.execute_reply.started":"2024-12-16T19:46:15.981269Z","shell.execute_reply":"2024-12-16T19:46:16.260664Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nbatch_size = 8\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ndev_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:17.889548Z","iopub.execute_input":"2024-12-16T19:46:17.890310Z","iopub.status.idle":"2024-12-16T19:46:17.895111Z","shell.execute_reply.started":"2024-12-16T19:46:17.890258Z","shell.execute_reply":"2024-12-16T19:46:17.894267Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"for batch in train_dataloader:\n    print(batch[\"input_ids\"].shape)  # (batch_size, max_length)\n    print(batch[\"attention_mask\"].shape)  # (batch_size, max_length)\n    print(batch[\"labels\"].shape)  # (batch_size,)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:19.411965Z","iopub.execute_input":"2024-12-16T19:46:19.412936Z","iopub.status.idle":"2024-12-16T19:46:19.436387Z","shell.execute_reply.started":"2024-12-16T19:46:19.412899Z","shell.execute_reply":"2024-12-16T19:46:19.435624Z"}},"outputs":[{"name":"stdout","text":"torch.Size([8, 256])\ntorch.Size([8, 256])\ntorch.Size([8])\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from transformers import AdamW, get_scheduler\n\noptimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n\n# Scheduler\nnum_training_steps = len(train_dataloader) * 5  # 5 epochs\nscheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:21.337201Z","iopub.execute_input":"2024-12-16T19:46:21.337992Z","iopub.status.idle":"2024-12-16T19:46:21.346736Z","shell.execute_reply.started":"2024-12-16T19:46:21.337958Z","shell.execute_reply":"2024-12-16T19:46:21.345829Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport json\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nnum_epochs = 5\ntrain_metrics = []\nval_metrics = []\nbest_val_accuracy = 0  # Initialize best validation accuracy\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    total_correct_train = 0\n    total_train_samples = 0\n    loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n    \n    for batch in loop:\n        # Move batch to GPU\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        # Forward pass\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n        \n        # Compute training accuracy\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        total_correct_train += (predictions == batch[\"labels\"]).sum().item()\n        total_train_samples += batch[\"labels\"].size(0)\n        \n        # Update progress bar\n        loop.set_postfix(loss=loss.item())\n    \n    train_accuracy = total_correct_train / total_train_samples\n    avg_train_loss = total_loss / len(train_dataloader)\n    \n    # Evaluate on dev set\n    model.eval()\n    total_correct_val = 0\n    total_val_samples = 0\n    total_val_loss = 0\n    \n    with torch.no_grad():\n        for batch in dev_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            total_correct_val += (predictions == batch[\"labels\"]).sum().item()\n            total_val_samples += batch[\"labels\"].size(0)\n    \n    val_accuracy = total_correct_val / total_val_samples\n    avg_val_loss = total_val_loss / len(dev_dataloader)\n    \n    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss}, Train Accuracy: {train_accuracy}\")\n    print(f\"Validation Accuracy: {val_accuracy}, Validation Loss: {avg_val_loss}\")\n    \n    # Save metrics for each epoch\n    train_metrics.append({'epoch': epoch+1, 'train_loss': avg_train_loss, 'train_accuracy': train_accuracy})\n    val_metrics.append({'epoch': epoch+1, 'val_loss': avg_val_loss, 'val_accuracy': val_accuracy})\n    \n    # Save model if current validation accuracy is higher than best\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        torch.save({\n            'epoch': epoch+1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_metrics': train_metrics,\n            'val_metrics': val_metrics\n        }, 'best_model_checkpoint.pt')\n        print(f\"New best model saved at epoch {epoch+1} with validation accuracy: {val_accuracy}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T19:46:23.655519Z","iopub.execute_input":"2024-12-16T19:46:23.655872Z","iopub.status.idle":"2024-12-16T20:42:00.898168Z","shell.execute_reply.started":"2024-12-16T19:46:23.655839Z","shell.execute_reply":"2024-12-16T20:42:00.896978Z"}},"outputs":[{"name":"stderr","text":"Epoch 1: 100%|██████████| 1744/1744 [06:26<00:00,  4.52it/s, loss=0.896]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Train Loss: 0.7196490515211443, Train Accuracy: 0.6825317181564046\nValidation Accuracy: 0.6752651189452565, Validation Loss: 0.7520538472508674\nNew best model saved at epoch 1 with validation accuracy: 0.6752651189452565\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 1744/1744 [06:24<00:00,  4.53it/s, loss=0.863] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Train Loss: 0.5362882591151726, Train Accuracy: 0.7737079779227296\nValidation Accuracy: 0.6938950988822012, Validation Loss: 0.7167064750937903\nNew best model saved at epoch 2 with validation accuracy: 0.6938950988822012\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 1744/1744 [06:25<00:00,  4.53it/s, loss=0.84]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Train Loss: 0.37286811994298985, Train Accuracy: 0.8551358325568059\nValidation Accuracy: 0.7082258526798509, Validation Loss: 0.8191769493432626\nNew best model saved at epoch 3 with validation accuracy: 0.7082258526798509\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 1744/1744 [06:24<00:00,  4.54it/s, loss=0.337]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Train Loss: 0.23881771753530376, Train Accuracy: 0.911332520966239\nValidation Accuracy: 0.7211235310977357, Validation Loss: 0.9115095950334889\nNew best model saved at epoch 4 with validation accuracy: 0.7211235310977357\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 1744/1744 [06:24<00:00,  4.54it/s, loss=0.0965] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Train Loss: 0.14477273199836502, Train Accuracy: 0.9529066016772991\nValidation Accuracy: 0.7176841501862998, Validation Loss: 1.048955521634748\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6: 100%|██████████| 1744/1744 [06:25<00:00,  4.53it/s, loss=0.0535] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6, Train Loss: 0.11711608170005151, Train Accuracy: 0.9631567629560605\nValidation Accuracy: 0.7176841501862998, Validation Loss: 1.048955521634748\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7: 100%|██████████| 1744/1744 [06:25<00:00,  4.53it/s, loss=0.011]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7, Train Loss: 0.11787813600876855, Train Accuracy: 0.962941724607555\nValidation Accuracy: 0.7176841501862998, Validation Loss: 1.048955521634748\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8: 100%|██████████| 1744/1744 [06:25<00:00,  4.53it/s, loss=0.0739] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8, Train Loss: 0.11747691536337189, Train Accuracy: 0.9627983657085514\nValidation Accuracy: 0.7176841501862998, Validation Loss: 1.048955521634748\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9:   5%|▍         | 82/1744 [00:18<06:09,  4.50it/s, loss=0.019]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 30\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     33\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:647\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[1;32m    646\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[0;32m--> 647\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    650\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report\n\nmodel.eval()\ntotal_correct_val = 0\ntotal_val_samples = 0\ntotal_val_loss = 0\n\n# Lists to store all true labels and predictions for calculating metrics\nall_true_labels = []\nall_predictions = []\n\nwith torch.no_grad():\n    for batch in dev_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        total_val_loss += loss.item()\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        total_correct_val += (predictions == batch[\"labels\"]).sum().item()\n        total_val_samples += batch[\"labels\"].size(0)\n\n        # Store true labels and predictions\n        all_true_labels.extend(batch[\"labels\"].cpu().numpy())\n        all_predictions.extend(predictions.cpu().numpy())\n\nval_accuracy = total_correct_val / total_val_samples\navg_val_loss = total_val_loss / len(dev_dataloader)\n\n# Calculate precision, recall, and F1-score\nprecision, recall, f1_score, _ = precision_recall_fscore_support(all_true_labels, all_predictions, average='weighted')\n\n# Generate classification report\nclass_report = classification_report(all_true_labels, all_predictions)\n\nprint(f\"Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}\")\nprint(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(class_report)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T20:47:59.580774Z","iopub.execute_input":"2024-12-16T20:47:59.581606Z","iopub.status.idle":"2024-12-16T20:48:27.213471Z","shell.execute_reply.started":"2024-12-16T20:47:59.581571Z","shell.execute_reply":"2024-12-16T20:48:27.212692Z"}},"outputs":[{"name":"stdout","text":"Validation Accuracy: 0.7177, Validation Loss: 1.0490\nPrecision: 0.7106, Recall: 0.7177, F1-Score: 0.7119\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.73      0.63      0.68       795\n           1       0.78      0.87      0.82      1736\n           2       0.57      0.51      0.54       958\n\n    accuracy                           0.72      3489\n   macro avg       0.69      0.67      0.68      3489\nweighted avg       0.71      0.72      0.71      3489\n\n","output_type":"stream"}],"execution_count":23}]}